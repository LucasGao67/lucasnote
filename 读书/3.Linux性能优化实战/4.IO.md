[toc]

# 1.文件系统

> Linux一切皆文件

文件系统详解：https://juejin.im/post/5b8ba9e26fb9a019c372e100

磁盘和文件系统的区别

- 磁盘为系统听过了最基本的持久化存储
- 文件系统在磁盘的基础上，提供了一个用来管理文件的树状结构。

## 文件系统入门

一个文件数据出了文件中实际内容外，还有一个其他的属性，比如文件权限和群组、时间、拥有者等。

文件系统通常会将这两部分数据分别存放在不同的区块[^1]，权限和属性放到`inode`中,至于实际数据则放到`data block`区块中。除此，还有一个`superblock`会记录整个文件系统的信息，包括`inode`和`block`的总量，使用量、剩余量等。

具体如下

- superblock:记录此 filesystem 的整体信息，包括inode/block的总量、使用量、剩余量， 以 及文件系统的格式与相关信息等;
- inode:记录文件的属性，一个文件占用一个inode，同时记录此文件的数据所在的 block 号 码;
- block:实际记录文件的内容，若文件太大时，会占用多个 block 。



每个`inode`和`block`都有自己的编号，inode作为入口，包含数据所在的block信息，按照索引可以全部取出。

如下图

![image-20200618134335189](http://picgo.vipkk.work/20200618134340.png)

上图就是索引式存储，ext4就是这种。

还有一个fat格式，它是没有inode的，大概结构如下，具体可以自行了解

<img src="http://picgo.vipkk.work/20200618134447.png" alt="image-20200618134447435" style="zoom:50%;" />



## inode

我们知道inode存储的是文件内容位置的具体block地址，但是我们有很多比较大的文件，记得上大学的时候，我们经常看的电影大一点的都有4G，inode记录一个block号码需要花费4Byte,那么这$G如果都记录的话 就需要   10^6 个block了，这是不能接受的，因为一个inode也就128byte, (ext4是256byte)。所以我们把inode记录block号码的区域定义为 12个直接、一个间接、一个双间接、一个三间接，如图：

<img src="http://picgo.vipkk.work/20200620011912.png" alt="image-20200620011911900" style="zoom:50%;" />



## block group

<img src="http://picgo.vipkk.work/20200620012122.png" alt="image-20200620012122458" style="zoom:50%;" />



## Superblock

它是记录整个filesystem相关信息的地方，他包含的信息有

>- block和inode的总量
>- 未使用的inode 、block数量
>- block和inode的大小
>- 系统挂载时间
>- 系统是否被挂载

我们可以使用`dumpe2fs`查看相关信息

<img src="http://picgo.vipkk.work/20200620012844.png" alt="image-20200620012844597" style="zoom:50%;" />

<img src="http://picgo.vipkk.work/20200620012938.png" alt="image-20200620012938241" style="zoom:50%;" />



除上述信息外，我们还可以发现 几乎每个block group 中都有一个 backup superblock,这是位了做备份，这样便于恢复。



## dentry(directory entry)

之前我们讨论了inode可以记录文件，除此之外还有一个目录项，用来记录文件名字、索引接地那与其他目录项之间的关联关系。多个关联的目录项，就构成了文件系统的目录结构。



不同于inode，dentry 是内核维护的内存数据结构。



为什么需要dentry内，我们设想下，如果每次修改目录或者我们设置软连接都写db的话，那性能会很差，所以可以利用dentry做缓存，其具体关系对应如图

<img src="http://picgo.vipkk.work/20200620014616.png" alt="img" style="zoom:80%;" />



## VFS （虚拟文件系统）

目录项（dentry）、索引节点(incode)、逻辑块（block)和超级块（superblock）构成了Linux文件系统的四大基本元素。不过，为了支持不同的文件系统，Linux内核在用户进程和文件系统之间，又引入了一个抽象层，就是VFS(virtual file system)。

> 抽象大家应该都懂
>
> VFS的抽象呢 也就算定义一组标准结构和jieko0u，这样用户进程和内核中的子系统就不需要关心文件系统具体的内部实现了

> 设计一个系统的终极目标往往就是要找到原子操作，一旦锁定了原子操作，设计工作就会变得简单而有序。“文件”作为一个抽象概念，其原子操作非常简单，只有读和写，这无疑是一个非常好的模型。通过这个模型，API的设计可以化繁为简，用户可以使用通用的方式去访问任何资源，自有相应的中间件做好对底层的适配。[^2]

VFS架构如下

![img](http://picgo.vipkk.work/20200620015152.png)

通过这张图，你可以看到，在 VFS 的下方，Linux 支持各种各样的文件系统，如 Ext4、XFS、NFS 等等。按照存储位置的不同，这些文件系统可以分为三类。

- 第一类是基于磁盘的文件系统，也就是把数据直接存储在计算机本地挂载的磁盘中。常见的 Ext4、XFS、OverlayFS 等，都是这类文件系统。
- 第二类是基于内存的文件系统，也就是我们常说的虚拟文件系统。这类文件系统，不需要任何磁盘分配存储空间，但会占用内存。我们经常用到的 /proc 文件系统，其实就是一种最常见的虚拟文件系统。此外，/sys 文件系统也属于这一类，主要向用户空间导出层次化的内核对象。
- 第三类是网络文件系统，也就是用来访问其他计算机数据的文件系统，比如 NFS、SMB、iSCSI 等。



## 文件系统IO

文件读写方式的各种差异，导致 I/O 的分类多种多样。最常见的有，缓冲与非缓冲 I/O、直接与非直接 I/O、阻塞与非阻塞 I/O、同步与异步 I/O 等。 接下来，我们就详细看这四种分类。



### 缓冲 I/O 与非缓冲 I/O

> 根据是否利用标准库缓存，可以把文件 I/O 分为缓冲 I/O 与非缓冲 I/O

- 缓冲 I/O，是指利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件。
- 非缓冲 I/O，是指直接通过系统调用来访问文件，不再经过标准库缓存

注意，这里所说的“缓冲”，是指标准库内部实现的缓存。比方说，你可能见到过，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来。无论缓冲 I/O 还是非缓冲 I/O，它们最终还是要经过系统调用来访问文件。



### 直接 I/O 与非直接 I/O

> 根据是否利用操作系统的页缓存，可以把文件 I/O 分为直接 I/O 与非直接 I/O。

- 直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件。
- 非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘。

想要实现直接 I/O，需要你在系统调用中，指定 O_DIRECT 标志。如果没有设置过，默认的是非直接 I/O。不过要注意，直接 I/O、非直接 I/O，本质上还是和文件系统交互。如果是在数据库等场景中，你还会看到，跳过文件系统读写磁盘的情况，也就是我们通常所说的裸 I/O。

### 阻塞 I/O 和非阻塞 I/O

> 根据应用程序是否阻塞自身运行，可以把文件 I/O 分为阻塞 I/O 和非阻塞 I/O

- 所谓阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务。
- 所谓非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果。

比方说，访问管道或者网络套接字时，设置 O_NONBLOCK 标志，就表示用非阻塞方式访问；而如果不做任何设置，默认的就是阻塞访问。



### 同步I/O和异步 I/O

> 根据是否等待响应结果，可以把文件 I/O 分为同步和异步 I/O

- 所谓同步 I/O，是指应用程序执行 I/O 操作后，要一直等到整个 I/O 完成后，才能获得 I/O 响应。

- 所谓异步 I/O，是指应用程序执行 I/O 操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次 I/O 完成后，响应会用事件通知的方式，告诉应用程序。

举个例子，在操作文件时，如果你设置了 O_SYNC 或者 O_DSYNC 标志，就代表同步 I/O。如果设置了 O_DSYNC，就要等文件数据写入磁盘后，才能返回；而 O_SYNC，则是在 O_DSYNC 基础上，要求文件元数据也要写入磁盘后，才能返回。

再比如，在访问管道或者网络套接字时，设置了 O_ASYNC 选项后，相应的 I/O 就是异步 I/O。这样，内核会再通过 SIGIO 或者 SIGPOLL，来通知进程文件是否可读写。







# 2. 磁盘IO 



## 通用块层

和VFS类似，为了减少不同设备的差异带来的影响，Linux通用一个统一的通用块层，来管理不同的块设备。

他主要有2个功能

- 跟VFS类似，作为抽象。向上提供标准的接口。想下管理不同设备的驱动程序
- 给文件系统和应用程序发来的I/O请求排队，并通过重新排序、请求合并等方式，提高磁盘读写效率。这也就是IO调度

### IO调度

> 有四种算法：NONE、NOOP、CFQ(CFS)、Deadline



NONE: 裸调度，就是完全不使用任何调度，这种调度一般用于虚拟机，这样的话 磁盘IO的调度完全交给物理机负责。

NOOP: 一个先入先出的队列，会对IO做一些基本的请求合并，常用于SSD磁盘。

CFQ[^3]: 完全公平调度器，默认使用。它为每个进程维护了一个I/O调度队列，并按照时间片来均匀分布每个进程的I/O请求。

Deadline：分别为读、写请求创建了不同的 I/O 队列，可以提高机械磁盘的吞吐量，并确保达到最终期限（deadline）的请求被优先处理。DeadLine 调度算法，多用在 I/O 压力比较重的场景，比如数据库等。



## I/O 栈

> 文件系统层-> 通过块层-> 设备层

![img](http://picgo.vipkk.work/20200620155626.png)



> 存储系统的IO,通常是整个系统最慢的一环，所以，Linux需要有很多机制来优化他们。



# 3. 磁盘性能指标

## 性能指标

- 使用率。磁盘处理I/O的时间占比，过高的使用率，通常意味着磁盘I/O存在性能瓶颈。
- 饱和度。磁盘处理I/O的繁忙程度。过道的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。
- IOPS。每秒的I/O请求数。
- 吞吐量。是值每秒的I/O请求的大小。
- 响应时间，是指I/O请求从发出到收到相应的间隔时间。

因地制宜，针对不同场景观察不同的指标



## 指标观测

### 整体概览：iostat[^5]

![image-20200620164854978](http://picgo.vipkk.work/20200620164855.png)



> loop[^4]
>
> 一个文件虚拟化成了设备

这里我们需要关注的几个指标

- %util, 磁盘IO使用率
- r/s、w/s，：IOPS。分别为read & write
- rkB/s、wkB/s ：吞吐量。
- r_await 、 w_await ：响应时间

其他指标解释

![img](http://picgo.vipkk.work/20200620170013.png)

### 单个进程观测：pidstat、iotop[^6]

pidstat 输出

<img src="http://picgo.vipkk.work/20200620170541.png" alt="image-20200620170541889" style="zoom:50%;" />

iotop输出

<img src="http://picgo.vipkk.work/20200620170520.png" alt="image-20200620170520563" style="zoom:50%;" />





# 4. 实战思考

## 被写磁盘拖慢的CPU

现象1：top

![image-20200620180212748](http://picgo.vipkk.work/20200620180212.png)

可见CPU比较高，同时 iowait几乎打满。=> 进而继续分析IO



现象2：iostat

![image-20200620180651867](http://picgo.vipkk.work/20200620180652.png)

首先观察使用率，96%，几乎打满。

再然后 write 每秒写进入144MB，且有1000左右的io等待。



现象3：pidstat

![image-20200620181125407](http://picgo.vipkk.work/20200620181125.png)



可见此时只有进程 20316在疯狂输出。

现象4：strace

<img src="http://picgo.vipkk.work/20200620181344.png" alt="image-20200620181344757" style="zoom:50%;" />



通过跟踪进程，我们可以发现这些信息

1. 该进程在疯狂write, 每次写 300m左右
2. write的文件是logtest.txt

> stat的文件名是 `name + . + 数字`,这种一般用于日志。

至此我们可以通过grep 或者其他手段找到文件中在疯狂输出的地方即可。



现象5：lsof[^7]

上面已经定位到了，不过我们还可以使用`lsof`来验证下（list open file）

<img src="http://picgo.vipkk.work/20200620181943.png" alt="image-20200620181943785" style="zoom:50%;" />

这里可以看到打开了这个文件，并在写入。

具体各个含义可以参考[^7]



## 被写磁盘拖慢的CPU2

这次同样CPU升高，iowait升高，

![image-20200620184751169](http://picgo.vipkk.work/20200620184751.png)

但是strace找不到具体的文件。

可是我们实实在在看到了CPU的升高，以及iowait，这个时候我们就可以想下是不是短时进程或者线程导致的

<img src="http://picgo.vipkk.work/20200620190009.png" alt="image-20200620190009487" style="zoom:100%;" />

![image-20200620190042755](http://picgo.vipkk.work/20200620190042.png)



通过watch 可以看到 SPID有一个一直在变



回到top

`top -H -p 20890`

![image-20200620190328344](http://picgo.vipkk.work/20200620190328.png)

同样可以观察到一个PID一直在变化，且它的CPU消耗最高

这时基本可以断定，进程开启线程处理，处理完了回收，且线程运行时间较短。



这个时候基本上就可以去找代码查看做了什么开启线程的地方



文件定位

除此之外，还可以通过filetop he opensnoop来具体查看下那个文件在被操作[^8]，具体见作者文章，因笔者没用那种方式，略过。



## 思考与总结

工具的个人总结

strace 

```
strace -f 可以把线程数据打印出来，针对第二个实战例子可用
```

lsof

```
lsof -w 忽略错误
```

跟踪子进程

```
pstree -apt PID 可以查看所有进程下的所有线程，
```

 这里我们基本就可以定位到线程本地，再然后呢 这时候就可以去查线程问题了，如果不行 在使用opensnoop。





# 5. 工具思路罗列

> IO相关工具如下

![img](http://picgo.vipkk.work/20200621000721.png)

![img](http://picgo.vipkk.work/20200621000702.png)

![img](http://picgo.vipkk.work/20200621000646.png)



## 思路

```
top
pidstat / iostat
strace / lsof
```





# 6. 优化思路

待完善，这块缺真正实战





[^1]: TODO 补充区块的概念，是否都在磁盘中：是，区块存在在内存中
[^2]: 参考： https://juejin.im/post/5b8ba9e26fb9a019c372e100
[^3]: https://en.wikipedia.org/wiki/Completely_Fair_Scheduler
[^4]: https://www.man7.org/linux/man-pages/man4/loop-control.4.html
[^5]: https://man7.org/linux/man-pages/man1/iostat.1.html
[^6]: https://man7.org/linux/man-pages/man8/iotop.8.html
[^7]: https://man7.org/linux/man-pages/man8/lsof.8.html
[^8]: https://time.geekbang.org/column/article/78409


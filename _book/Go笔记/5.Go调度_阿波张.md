[toc]

---

# 0. 预备知识

## 1. 寄存器

我们一般用到的寄存器有三种

1. 通用寄存器

   rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8, r9, r10, r11, r12, r13, r14, r15寄存器。CPU对这16个通用寄存器的用途没有做特殊规定。
   
   但是这些寄存器有一些默认用途，
   
   1. 在传参的时候，前6个参数分别为：**rax, rbx, rcx, rdx, rsi, rdi**。
   
   2. rsp 栈顶寄存器 & rbp 栈基址寄存器
   
      这2个寄存器都跟函数调用栈相关。其中rsp一般存在栈顶的地址，而rbp是栈帧起始地址。编译器一般用这2个寄存器来获取函数局部变量或者参数
   
2. 指令寄存器、程序计算寄存器

   rip寄存器，它用来存放下一条即将执行的指令的地址。

3. 段寄存器

   fs和gs寄存器，在go中使用fs寄存器实现线程本地存储。



## 2. 内存

> 1. 内存的单元为字节，每一个字节都有一个地址
> 2. 变量连续存储：**何大于一个字节的变量在内存中都存储在相邻连续的的几个内存单元之中；**
> 3. 大端&小端
>    1. 大端：高字节-> 低地址
>    2. 小端：高字节-> 高地址





<img src="http://picgo.vipkk.work/20200513204742.png" alt="image-20200513204742896" style="zoom:50%;" />

 说明：

 rsp 指的是栈顶是已经使用的地址。非下一条地址。



## 3. 函数调用栈

> 函数是以栈的方式调用的。



程序运行时布局图：



<img src="http://picgo.vipkk.work/20200511220555.png" alt="img" style="zoom:100%;" />

进程在虚拟地址的布局如上。一个进程把内存分为了4个部分：

1. 代码区： 包括被CPU执行的机器代码和只读数据比如字符串常量。一旦加载完成就不会再变化
2. 数据区：包括程序的全局变量和静态变量（c语言有静态变量，而go没有）。一旦加载完成就不会再变化
3. 堆：动态分配的内存在堆中。
4. 栈：**函数调用栈**

---

下面重点说函数调用栈

它在函数中扮演着重要的角色

1. 保存 函数中的局部变量
2. 传递 在函数调用中传递参数
3. 返回 把函数的返回值返回
4. 保存 函数的返回地址

每个函数在执行过程中都需要使用一块内存来保存上述的这些值，我们称这块栈内存为**栈帧（stack frame）**。当发生函数调用的时候，被调用者不能覆盖调用者的栈帧，所以需要把调用者的栈帧`push`到栈上，等调用完成再pop。

另外，在AMD64 Linux平台，栈是从高向低方向生成的。其中就使用了上面提到的2个寄存器

- rsp
- rbp

举例，假设有如下调用关系`A()->B()->C()`,则有如下的调用关系

<img src="http://picgo.vipkk.work/20200512004903.png" alt="image-20200512004857407"  />

需要注意：

1. 函数调用时，参数和返回值都是存放在调用者的栈帧中。（这个会影响到行程的汇编代码）
2. go语言把参数和返回值都是放在栈上。（gcc 是吧参数和返回值放到寄存器中）

当C、B函数运行完，A调用D得到如图

![image-20200512010034024](http://picgo.vipkk.work/20200512010034.png)

如上图，D覆盖了之前B、C的内存。

> 正因为栈的内存会被覆盖，所以在C语言中才不能返回局部变量的指针。但是Go因为有内存逃逸，则不会有这样的问题。



key note

1. 每个进程地址一致是靠虚拟内存机制保证的。
2. caller save,callee save



## 4. 汇编指令

> https://mp.weixin.qq.com/s?__biz=MzU1OTg5NDkzOA==&mid=2247483693&idx=1&sn=e5398ae82e2f3484bea5e8858b1a9cd7&scene=19#wechat_redirect

## 5.  Go 汇编语言

> go中的runtime有部分代码是用汇编写的。但是它的汇编语言并非针对特定体系结构的汇编代码，而是go语言引入的的伪汇编plan9.
>
> TODO: go blog about plan9

go汇编和AT&T 差不多，但是也有区别，下面主要对其做说明。（因为后续调度分析的时候需要看汇编）



寄存器映射

![image-20200512094901725](http://picgo.vipkk.work/20200512094901.png)



除此go还引入几个虚拟寄存器：（所谓虚拟寄存器，就是没有任何硬件寄存器与之对应）。这些寄存器一般用来存放内存地址，引入他们的目的是为了方便程序员和编译器 用来定位内存中的代码和数据。



**FP虚拟寄存器**

主要用来引用函数参数。前面提到过go的参数都在栈上。所以引入FP可以方便我们获取到参数地址。

比如可以使用`firstarg+0(FP)`来引用调用者传递来的第一个参数，用`secondarg+8(FP)`来引用第二个参数。这里``firstarg`和`secondarg`都是无意义的符号，编译器不关心也不解读。举例：

go 中有个gogo函数，接受一个gobuf指针

```go
// src/runtime/stubs.go:129
func gogo(buf *gobuf)
```

对应的汇编部分如下

```asm
// func gogo(buf *gobuf)
// restore state from Gobuf; longjmp
TEXT runtime·gogo(SB), NOSPLIT, $16-8
	MOVQ	buf+0(FP), BX		// gobuf -> BX
	MOVQ	gobuf_g(BX), DX // gp.sched.g = dx
	...
```

由上，可以看到通过FP获取到参数。回想之前的栈，FP并不在当前的函数栈帧上，其关系如图。

![image-20200512162439879](http://picgo.vipkk.work/20200512162440.png)



SB虚拟寄存器：

上面的示例代码中有`TEXT runtime·gogo(SB), NOSPLIT, $16-8`。其中SB保存的是程序地址空间的的起始地址。之前栈中代码区的地方。这个SB寄存器保存的值就是代码区的起始地址，它主要用来定位全局符号。Go汇编中的函数定义、函数调用、全局变量以及对其引用都会用到这个SB寄存器。



函数的其他定义：

1. `TEXT runtime·gogo(SB)`:指明在代码区定义了一个名字叫 gogo的全局函数，该函数属于runtime包
2. `NOSPLIT`:指示编译器不要再这个函数中插入检查栈是否溢出的代码。（TODO）
3. `$16-8`：16代表函数栈帧为16字节，8代码函数的参数和返回值一共8个字节。



## 6. 函数调用过程

> https://mp.weixin.qq.com/s?__biz=MzU1OTg5NDkzOA==&mid=2247483723&idx=1&sn=772960aa0d5ae4aa6921e9ff43fcb99f&scene=19#wechat_redirect

## 7. 系统调用

> 操作内核

https://mp.weixin.qq.com/s/YPiYNPa3xVD9Il1HeB5pTw

## 8. 操作系统的线程和线程调度

> 要深入理解goroutine的调度器，就需要对操作系统线程有个大致的了解，因为go的调度系统是建立在操作系统线程之上的，所以接下来我们对其做一个简单的介绍。
>
> 很难对线程下一个准确且易于理解的定义，特别是对于从未接触过多线程编程的读者来说，要搞懂什么是线程可能并不是很容易，所以下面我们抛开定义直接从一个C语言的程序开始来直观的看一下什么是线程。之所以使用C语言，是因为C语言中我们一般使用pthread线程库，而使用该线程库创建的用户态线程其实就是Linux操作系统内核所支持的线程，它与go语言中的工作线程是一样的，这些线程都由Linux内核负责管理和调度，然后go语言在操作系统线程之上又做了goroutine，实现了一个**二级线程**模型。

什么时候发生调度

1. 用户使用系统调用
2. 硬件中断尤其是时钟中断

线程保存着程序运行的上下文

	-  寄存器中的值
	-  下一条指令
	-  栈

所以当进行线程切换的时候需要把这些都保存下



参考：

1. [进程/线程上下文切换会用掉你多少CPU](https://zhuanlan.zhihu.com/p/79772089)
2. [协程究竟比线程能省多少开销？](https://zhuanlan.zhihu.com/p/80037638)

## 9. 线程本地存储

> **线程本地存储又叫线程局部存储，其英文为Thread Local Storage，简称TLS**

TLS 在用户侧代码是一个变量，但是在编译层次确实2个地址。所以用户使用的时候就可以使用一个变量访问2个地址。



我们需要知道fs段基址是多少，虽然我们可以用gdb命令查看fs寄存器的值，但fs寄存器里面存放的是段选择子（segment selector）而不是该段的起始地址



---





# 1. Goroutine调度器

## 1. goroutine简介

首先为什么Go语言为什么要引入协程Goroutine呢。

这主要是因为操作系统的线程太重了，具体表现在

1. 创建和切换太重：所有的线程切换都需要系统调用进入内核，而进入内核的性能代价比较大
2. 内存使用太重：一方面，为了尽量避免极端情况下操作系统线程栈的溢出，内核在创建操作系统线程时默认会为其分配一个较大的栈内存（虚拟地址空间，内核并不会一开始就分配这么多的物理内存），然而在绝大多数情况下，系统线程远远用不了这么多内存，这导致了浪费；另一方面，栈内存空间一旦创建和初始化完成之后其大小就不能再有变化，这决定了在某些特殊场景下系统线程栈还是有溢出的风险。

> 可以使用 `ulimit -a`命令查看本机的栈大小。如下图，栈的代销为8192kB=8MB。
>
> ![image-20200512220534183](http://picgo.vipkk.work/20200512220534.png)

所有go引进了Goroutine，相比较旧特别轻量

1. Goroutine是用户态线程，创建和切换都可以再用户态完成，无需进入内核，开销比较小。
2. Goroutine默认大小是2k,相比操作系统线程的8M，简直是太小了。不过虽然协程栈比较小，但是支持自动扩容和收缩，所以也不用担心内存不够用或者浪费。



### Goroutine的调度简介

> 所谓对协程的调度，其实就是Go 按照一定的算法逻辑挑选出合适的 Goroutine并放到CPU上运行的过程。
>
> 在go中 有专门负责调度的代码，我们成为goroutine调度器。

大概逻辑如下：

```go

for i:=0;i<N;i++{ // 创建N个操作系统执行schedule函数
  create_os_thread(schedule)
}

func schedule(){
  for{
    g := find_a_runnable_g_from_M_gs()
    run_g(g)
    save_status_of_g(g)
  }
}
```

总结：也就算程序开始创建N个线程去执行`schedule`函数。

每个`schedule`函数 回去寻找一个可以执行的g，然后执行g，在保存g的状态，循环往复。



### 调度器相关的数据结构源码概述

> 无他，所谓调度，本质上都是对寄存器的一些操作。
>
> 只不过线程调度是在内核态，由操作系统完成。
>
> 而协程调度是在用户态，由go自己完成。

如上，go自己要完成对协程的调度。那么go就需要引入一个数据结构来保存协程中的一些值（寄存器等）。

这就是`g`。

	1. 当协程被调离的时候，它的相关信息都保存在g中
 	2. 当协程被恢复的时候，调度器又可以通过g将其状态还原。

现在有了`g`,那么对`g`进行调度就需要调度器。不过调度器本身也是需要保存自身状态的，所以这就有了`schedt`结构体。因为一个go程序只有一个调度器，所以schedt全局唯一。

有了调度器，有了协程，我们还需要一个队列来保存可以运行的g，这就是局部运行队列。考虑到并发，go为每个`p`都配了一个局部运行队列，他保存着一系列等待执行的`g`.那么什么是`p`呢，它是processor的简写，是个掮客，协调着工作线程和g之间的关系。

除了这些，还有一个结构，它需要跟工作线程打交道，同时接收`g`，这就是`m`。每个工作线程都有唯一的一个`m`结构体与之对应，m结构体记录着线程的的栈信息等。

除此，还有一个全局运行队列，它是一个保存在`schedt`中的运行队列。具体后面再说。

下面是上面几种结构的对应关系：

<img src="http://picgo.vipkk.work/20200513211350.png" alt="image-20200513211350567" style="zoom:40%;" />



**一个细节**

前面我们提到m会跟工作线程绑定，但是我们有多个m，那么是如何做到区分的呢。记得上面的伪代码吗，我们调度的程序是一致的，那怎么区分呢。这就用到了前面提到的本地存储，threadlocal。



具体结构体代码

TODO 因为太多不在这里展示



## 2. 调度器初始化

> 本章将以一个hello world程序为例，通过跟踪其从启动到退出的这一完整的运行流程来分析Go语言调度器的初始化、goroutine的创建与退出、工作线程的调度循环以及goroutine的切换等内容。
>
> ```go
> package main
> 
> import "fmt"
> 
> func main(){
>   fmt.Println("Hello World!")
> }
> ```
>
> 

这节我们主要来分析调度器的初始化。

> 任何一个编译型语言（不管是C、C++、go）所编写的程序在操作系统上运行都有以下几个阶段
>
> 1. 从磁盘上把可执行程序读入内存
> 2. 创建进程和主线程
> 3. 为主线程分配栈空间
> 4. 把用户在命令行输入的参数拷贝到主线程的栈上
> 5. 把主线程放在操作系统的运行队列等待被调度执行起来。
>
> 在主线程第一次被调度起来之前，栈如下图所示：
>
> <img src="http://picgo.vipkk.work/20200513221220.png" alt="image-20200513221220322" style="zoom:50%;" />





接下来我们从代码出发讲解下 调度器中的各个数据结构是怎么关联起来的





### 程序入口

首先我们需要找到梦（代码）开始的地方。



首先我们编译程序，然后通过gdb调试`go build -o hello;gdb hello`,然后输入`info files`查看调试信息

![image-20200515215956835](http://picgo.vipkk.work/20200515215956.png)

可以发现入口在`0x454b70`,然后 在这里打断点`b *0x454b70`,可以发现程序入口的地方。对应代码如下，暂时不接受。

> 以下代码太多，建议结合源码食用

```assembly

// 梦开始的地方 runtime/rt0_linux_amd64.s:8
TEXT _rt0_amd64_linux(SB),NOSPLIT,$-8
	JMP	_rt0_amd64(SB)

TEXT _rt0_amd64(SB),NOSPLIT,$-8
	MOVQ	0(SP), DI	// argc
	LEAQ	8(SP), SI	// argv
	JMP	runtime·rt0_go(SB)
	
// runtime/asm_amd64.s	
TEXT runtime·rt0_go(SB),NOSPLIT,$0
	// copy arguments forward on an even stack
	MOVQ	DI, AX		// argc
	MOVQ	SI, BX		// argv
	SUBQ	$(4*8+7), SP		// 2args 2auto 16字节对齐
	ANDQ	$~15, SP
	MOVQ	AX, 16(SP)
	MOVQ	BX, 24(SP)

```

### g0初始化

在`rt0_go`中将要完成了我们程序启动所有的的初始化工作。

上面代码基本上就是将SP对齐，接着往下

```assembly
// create istack out of the given (operating system) stack.
	// _cgo_init may update stackguard.
	// g0 -> DI
	MOVQ	$runtime·g0(SB), DI

	LEAQ	(-64*1024+104)(SP), BX 	// BX = SP - 64 * 1024 + 104
	MOVQ	BX, g_stackguard0(DI) 	// g0.stackguard0 =  SP - 64 * 1024 + 104
	MOVQ	BX, g_stackguard1(DI) 	// g0.stackguard1 =  SP - 64 * 1024 + 104
	MOVQ	BX, (g_stack+stack_lo)(DI) 	// g0.stack.lo = SP - 64 * 1024 + 104
	MOVQ	SP, (g_stack+stack_hi)(DI) 	// g0.stack.l1 = SP
```

这里完成了对g0的初始化, 可见g0的栈大小为 64K。

此时g0与程序栈之间的关系如图

<img src="http://picgo.vipkk.work/20200515221806.png" alt="image-20200515221806358" style="zoom:40%;" />

> 注意，g0是全局变量，所以这时候g0的内存已经申请好了，在全局变量区（data+bss).



上面图的关系 我们可以通过gdb 打印下地址查看下

![image-20200515222801728](http://picgo.vipkk.work/20200515222801.png)

从图中可以看到 hi 指向了sp，其他亦然。

### m0初始化&绑定OS线程

g0的栈好之后，我们来看下m0的初始化。

跳过CPU型号检查和cgo初始化的代码，往下看

```assembly
	// 初始化m的tls。DI= &m0.tls, 取 m0的tls成员地址到DI寄存器
	LEAQ	runtime·m0+m_tls(SB), DI
	// 调用 settls设置 线程本地存储，settls 函数的参数在DI寄存器中。 之后，可以通过fs断寄存器找到 m.tls。
	CALL	runtime·settls(SB)

	// store through it, to make sure it works
	get_tls(BX) // 获取fs段基址并放入BX寄存器， 其实就是 m0.tls[1] 的地址，get_tls的代码由编译器生成
	MOVQ	$0x123, g(BX) // set m0.tls[0] = 0x123, 也就算 fs地址-8的内存位置。 g 代码－8
	MOVQ	runtime·m0+m_tls(SB), AX // AX = m0.tls[0]
	CMPQ	AX, $0x123 // 比较
	JEQ 2(PC)
	CALL	runtime·abort(SB) // 如果线程本地存储不能正常工作，退出程序
```

这段代码调用 settls 来实例化 m0.tls, tls就是对应之前提到thread local storage，目的是为了把m0这个数据结构和线程关联在一起，从而我们可以把m0当做线程的抽象。

其中settls函数就是把fs寄存器的地址设置为了tls[1]的地址。

继续：

```assembly
ok:
	// set the per-goroutine and per-mach "registers"
	get_tls(BX)
	LEAQ	runtime·g0(SB), CX // CX = g0
	MOVQ	CX, g(BX) // m0.tls[0]= g0
	LEAQ	runtime·m0(SB), AX // AX = &m0

  // 以下两行代码是把 m0和g0串了起来
	// save m->g0 = g0
	MOVQ	CX, m_g0(AX)
	// save m0 to g0->m
	MOVQ	AX, g_m(CX)
```

以上，在m0于主线程绑定之后，我们又通过指针把m0和g0关联在了一起。此时内存布局如下



![image-20200516160536516](http://picgo.vipkk.work/20200516160536.png)

可以看到，内存指针一致。

另外m0.tls[0] 存储的也是g0地址，tls[1]的地址其实是fs基地址（这个可以参考之前的tls章节）。

此时内存如下图：

<img src="http://picgo.vipkk.work/20200516172239.png" alt="image-20200516172239213" style="zoom:50%;" />



继续

```
	MOVL	16(SP), AX		// copy argc
	MOVL	AX, 0(SP)
	MOVQ	24(SP), AX		// copy argv
	MOVQ	AX, 8(SP)
	CALL	runtime·args(SB)
	CALL	runtime·osinit(SB)
	CALL	runtime·schedinit(SB) // 调度器初始化
```

m0初始化完成之后，开始对调度器初始化

### 调度器初始化

在这里就是go 语言了

```go
// The bootstrap sequence is:
//
//	call osinit
//	call schedinit
//	make & queue new G
//	call runtime·mstart
//
// The new G calls runtime·main.
func schedinit() {
	// raceinit must be the first call to race detector.
	// In particular, it must be done before mallocinit below calls racemapshadow.
	// getg 是个什么鬼，getg 由编译器实现，大概原理就是 通过fs 找到 tls[1], 再-8找到g0,也就算当前运行的g
	_g_ := getg() // _g_=&g0
	//  ......

  //设置最多启动10000个操作系统线程，也是最多10000个M
  sched.maxmcount = 10000

  // ......

  mcommoninit(_g_.m) //初始化m0，因为从前面的代码我们知道g0->m = &m0

  // ......

  sched.lastpoll = uint64(nanotime())
  procs := ncpu  //系统中有多少核，就创建和初始化多少个p结构体对象
  if n, ok := atoi32(gogetenv("GOMAXPROCS")); ok && n > 0 {
    procs = n //如果环境变量指定了GOMAXPROCS，则创建指定数量的p
  }
  if procresize(procs) != nil {//创建和初始化全局变量allp
    throw("unknown runnable goroutine during bootstrap")
  }

  // ......

}
```

以上我们对`schedinit`函数提取了下大纲。这次我们主要关注以下几点

1. getg()

   getg()是汇编实现的，大概就是从tls取出当前运行的g,在这里就是g0。

2. 设置最多线程个数为10000个，(对应内存也就是80000M ~= 80G)

3. mcommoninit

   1. 对m0
   2. 对p

#### mcommoninit

> 1. 对m0做另外的一些初始化
> 2. 创建和实例化p

```go
func mcommoninit(mp *m) {
	_g_ := getg()

	// g0 stack won't make sense for user (and is not necessary unwindable).
	if _g_ != _g_.m.g0 {
		callers(1, mp.createstack[:])
	}

	// 因为sched是一个全局变量，多个线程同时操作 sched会有并发问题，因此要先加锁，操作结束后再解锁。
	lock(&sched.lock)
	if sched.mnext+1 < sched.mnext {
		throw("runtime: thread ID overflow")
	}
	mp.id = sched.mnext // 给id赋值
	sched.mnext++       // m0的 id就是0，并且之后创建的m的id是递增的。
	checkmcount()       // 检查是否超过数量限制

	// random 初始化
	mp.fastrand[0] = uint32(int64Hash(uint64(mp.id), fastrandseed))
	mp.fastrand[1] = uint32(int64Hash(uint64(cputicks()), ^fastrandseed))
	if mp.fastrand[0]|mp.fastrand[1] == 0 {
		mp.fastrand[1] = 1
	}

	// 创建用于信号处理的goroutine ,gsignal，只是简单的从堆上分配一个g结构体对象，然后把栈设置好久返回了
	mpreinit(mp)
	if mp.gsignal != nil {
		mp.gsignal.stackguard1 = mp.gsignal.stack.lo + _StackGuard
	}

	// Add to allm so garbage collector doesn't free g->m
	// when it is just in a register or thread-local storage.
	// 把m 挂入全局链表 allm之中
	// 类比 m0.next = allm allm = m0
	mp.alllink = allm

	// NumCgoCall() iterates over allm w/o schedlock,
	// so we need to publish it safely.
	atomicstorep(unsafe.Pointer(&allm), unsafe.Pointer(mp))
	unlock(&sched.lock)

	// Allocate memory to hold a cgo traceback if the cgo call crashes.
	if iscgo || GOOS == "solaris" || GOOS == "illumos" || GOOS == "windows" {
		mp.cgoCallers = new(cgoCallers)
	}
}
```

这里主要对m0做了以下几个事情

1. 赋值id
2. 初始化m的随机数
3. 信号初始化（后续分析抢占调度的时候用得到）
4. 把m0自己放到allm全局变量里。

此时 allm 指向m0

![image-20200516175358260](http://picgo.vipkk.work/20200516175358.png)

#### procresize

```go
// Change number of processors. The world is stopped, sched is locked.
// gcworkbufs are not being modified by either the GC or
// the write barrier code.
// Returns list of Ps with local work, they need to be scheduled by the caller.
func procresize(nprocs int32) *p {
	old := gomaxprocs
	if old < 0 || nprocs <= 0 {
		throw("procresize: invalid arg")
	}
	if trace.enabled {
		traceGomaxprocs(nprocs)
	}

	// update statistics
	now := nanotime()
	if sched.procresizetime != 0 {
		sched.totaltime += int64(old) * (now - sched.procresizetime)
	}
	sched.procresizetime = now

	// Grow allp if necessary.
	if nprocs > int32(len(allp)) { // 初始化的时候 len(allp) = 0,因为目前还没有P
		// Synchronize with retake, which could be running
		// concurrently since it doesn't run on a P.
		lock(&allpLock)
		if nprocs <= int32(cap(allp)) {
			allp = allp[:nprocs]
		} else {
			nallp := make([]*p, nprocs)
			// Copy everything up to allp's cap so we
			// never lose old allocated Ps.
			copy(nallp, allp[:cap(allp)]) // 代码中途调整p的数量？
			allp = nallp
		}
		unlock(&allpLock)
	}

	// initialize new P's
	// 循环创建 nproc个p，并完成基本初始化
	for i := old; i < nprocs; i++ {
		pp := allp[i]
		if pp == nil {
			pp = new(p) // 调用内存分配器从堆上分配一个 struct p
		}
		pp.init(i) // 设置p的id，mcache等。
		atomicstorep(unsafe.Pointer(&allp[i]), unsafe.Pointer(pp))
	}

	_g_ := getg()
	if _g_.m.p != 0 && _g_.m.p.ptr().id < nprocs {
		// continue to use the current P
		_g_.m.p.ptr().status = _Prunning
		_g_.m.p.ptr().mcache.prepareForSweep()
	} else { // 初始化的时候执行这个分支
		// release the current P and acquire allp[0].
		//
		// We must do this before destroying our current P
		// because p.destroy itself has write barriers, so we
		// need to do that from a valid P.
		if _g_.m.p != 0 {
			if trace.enabled {
				// Pretend that we were descheduled
				// and then scheduled again to keep
				// the trace sane.
				traceGoSched()
				traceProcStop(_g_.m.p.ptr())
			}
			_g_.m.p.ptr().m = 0
		}
		_g_.m.p = 0
		_g_.m.mcache = nil
		p := allp[0]
		p.m = 0
		p.status = _Pidle
		// 把 p和m0关联起来
		acquirep(p)
		if trace.enabled {
			traceGoStart()
		}
	}

	// release resources from unused P's
	// 调整P的个数
	for i := nprocs; i < old; i++ {
		p := allp[i]
		p.destroy()
		// can't free P itself because it can be referenced by an M in syscall
	}

	// Trim allp.
	if int32(len(allp)) != nprocs {
		lock(&allpLock)
		allp = allp[:nprocs]
		unlock(&allpLock)
	}

	// 把所有空闲的P放入空闲链表
	// 它将除了p0的所有非空闲的P，放入P链表runnablePs，并返回procresize函数的调用者，
	// 并由其来调度这些P
	var runnablePs *p
	for i := nprocs - 1; i >= 0; i-- {
		p := allp[i]
		if _g_.m.p.ptr() == p { // allp[0] 跟m0 关联了，所有不放入
			continue
		}
		p.status = _Pidle
		// 如果P 是空闲的，则放入 idle里
		if runqempty(p) { // 通过 runqempty 判断p 的LRQ中是否包含G，
			pidleput(p) // 初始化除了allp[0]其他P 全部执行这个分支，分入空闲链表
		} else {
			// ??
			p.m.set(mget())
			p.link.set(runnablePs)
			runnablePs = p
		}
	}
	// 初始化一个随机的分配器
	stealOrder.reset(uint32(nprocs))
	var int32p *int32 = &gomaxprocs // make compiler check that gomaxprocs is an int32
	atomic.Store((*uint32)(unsafe.Pointer(int32p)), uint32(nprocs))
	return runnablePs
}

```

这个代码干了以下几个事情

1. 初始化全局变量allp
2. 填充allp
3. m0和allp[0]绑定
4. 除allp[0]之外的所有p放到 等待队列中

`allp是全局变量`

并且会把m0和allp[0]关联起来。

![image-20200516181153374](http://picgo.vipkk.work/20200516181153.png)

![image-20200516181924462](http://picgo.vipkk.work/20200516181924.png)

> 这里没太搞明白内存的对应关系

至此，m0,g0和p就关联在一起了。

如图：

<img src="http://picgo.vipkk.work/20200516181939.png" alt="image-20200516181939265" style="zoom:50%;" />



---

## 3. 创建 main goroutine

当现在我们的调度器已经搭建好了，就像我们的工厂监工已经就位了，就等着劳动者来干活了。

#### newproc--起源

继续代码

```assembly
// create a new goroutine to start program
  // 创建一个新的 goroutine来启动程序
	MOVQ	$runtime·mainPC(SB), AX		// entry
	PUSHQ	AX  // newproc 的第二个参数入栈，也就算goroutine需要执行的函数。 AX = funcval{runtime.main}
	PUSHQ	$0			// arg size // newporc 的第一个参数入栈，该参数表示 runtime.main 函数需要的参数大小。 因为 runtime.main没有参数，所以这里是0
	CALL	runtime·newproc(SB) // 创建 main goroutine
	POPQ	AX
	POPQ	AX

	// start this M
	CALL	runtime·mstart(SB) # 主线程进入调度循环，运行刚刚创建的goroutine

	CALL	runtime·abort(SB)	// mstart should never return
	RET

	// Prevent dead-code elimination of debugCallV1, which is
	// intended to be called by debuggers.
	MOVQ	$runtime·debugCallV1(SB), AX
	RET

DATA	runtime·mainPC+0(SB)/8,$runtime·main(SB)
GLOBL	runtime·mainPC(SB),RODATA,$8
```

这里，我们调用的是newproc, 他接受2个参数，先说第二个参数fn，新创建出来的goroutine将从fn这个函数开始执行，而这个fn函数可能也会有参数，newproc的第一个参数正是fn函数的参数以字节为单位的大小。

比如有以下代码

```go
func start(a, b, c int64) {
    ......
}

func main() {
    go start(1, 2, 3)
}
```

对应的汇编则是

```assembly
func main() {
    push 0x3
    push 0x2
    push 0x1
    runtime.newproc(24, start)
}
```

下面开始分析 newproc的代码

```go
// Create a new g running fn with siz bytes of arguments.
// Put it on the queue of g's waiting to run.
// The compiler turns a go statement into a call to this.
// Cannot split the stack because it assumes that the arguments
// are available sequentially after &fn; they would not be
// copied if a stack split occurred.
//go:nosplit
func newproc(siz int32, fn *funcval) {
	// 函数调用参数入栈顺序是从右向左，而且栈是从高地址向低地址增长的。
	// 注意：argp 指向fn函数的第一个参数，而不是newproc函数的参数
	// 参数fn在栈上的地址+8 的位置存放的fn函数的第一个参数。
	argp := add(unsafe.Pointer(&fn), sys.PtrSize)
	// 获取正在运行的g, 初始化时是 m0.g0
	gp := getg()
	// getcallerpc 返回一个地址，也就算调用newproc时由 call指令压栈的函数返回的资质
	// 对于new main来说，就指令后面的 POPQ AX 这条指令的地址
	pc := getcallerpc()
	// systemstack 的作用是切换到 g0栈执行 作为参数 的函数
	// 如果在g0上，就什么都不做
	systemstack(func() {
		newproc1(fn, argp, siz, gp, pc)
	})
}
```

`newproc`函数是对`newproc1`的一个包装，他主要是一些准备工作：

1. 获取fn函数的第一个地址
2. 函数栈切换到g0,对于我们这个初始化场景来说现在本来就在g0栈，所以不需要切换，然而这个函数是通用的，在用户的goroutine中也会创建goroutine，这时就需要进行栈的切换。

#### newproc1 进程创建在真正发起人

下面我么看下`newproc1`函数，他接受5个参数

```go
/**
1. fn, 新创建的goroutine需要执行的函数
2. argp, fn函数的第一个参数地址
3. narg, fn参数的大小
4. callergp 调用者的g
5. callerpc 调用者的下一行指令
*/
func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) {}
```

再次强调`newproc1`是在g0栈上执行的，这个函数是创建g非常重要的函数。很长很重要，我们接着分析

```go
// Create a new g running fn with narg bytes of arguments starting
// at argp. callerpc is the address of the go statement that created
// this. The new g is put on the queue of g's waiting to run.
func newproc1(fn *funcval, argp *uint8, narg int32, callergp *g, callerpc uintptr) {
    //因为已经切换到g0栈，所以无论什么场景都有 _g_ = g0，当然这个g0是指当前工作线程的g0
    //对于我们这个场景来说，当前工作线程是主线程，所以这里的g0 = m0.g0
    _g_ := getg()

    ......

    _p_ := _g_.m.p.ptr() //初始化时_p_ = g0.m.p，从前面的分析可以知道其实就是allp[0]
    newg := gfget(_p_) //从p的本地缓冲里获取一个没有使用的g，初始化时没有，返回nil
    if newg == nil {
         //new一个g结构体对象，然后从堆上为其分配栈，并设置g的stack成员和两个stackgard成员
        newg = malg(_StackMin)
        casgstatus(newg, _Gidle, _Gdead) //初始化g的状态为_Gdead
         //放入全局变量allgs切片中
        allgadd(newg) // publishes with a g->status of Gdead so GC scanner doesn't look at uninitialized stack.
    }
   
    ......
   
    //调整g的栈顶置针，无需关注
    totalSize := 4*sys.RegSize + uintptr(siz) + sys.MinFrameSize // extra space in case of reads slightly beyond frame
    totalSize += -totalSize & (sys.SpAlign - 1)                  // align to spAlign
    sp := newg.stack.hi - totalSize
    spArg := sp

    //......
   
    if narg > 0 {
         //把参数从执行newproc函数的栈（初始化时是g0栈）拷贝到新g的栈
        memmove(unsafe.Pointer(spArg), unsafe.Pointer(argp), uintptr(narg))
        // ......
    }
  .......
}
```

这段代码主要是从堆上为newg分配一个大小为2k的栈，将其放到全局变量allgs中，然后设置其stack。

然后将需要执行的参数从newproc函数的栈上拷贝到newg的栈上。此时

<img src="http://picgo.vipkk.work/20200517164534.png" alt="image-20200517164533978" style="zoom:50%;" />



![image-20200517170552345](http://picgo.vipkk.work/20200517170552.png)



目前为止，我们从堆上给g分配了一个大小为2k的栈，且初始化好了stack。并且我们把新生成的g放到了全局allgs中

#### 实例化sched--goexit的骚操作

我们继续

```go
 //把newg.sched结构体成员的所有成员设置为0
    memclrNoHeapPointers(unsafe.Pointer(&newg.sched), unsafe.Sizeof(newg.sched))
   
    //设置newg的sched成员，调度器需要依靠这些字段才能把goroutine调度到CPU上运行。
    newg.sched.sp = sp  //newg的栈顶
    newg.stktopsp = sp
    //newg.sched.pc表示当newg被调度起来运行时从这个地址开始执行指令
    //把pc设置成了goexit这个函数偏移1（sys.PCQuantum等于1）的位置，
    //至于为什么要这么做需要等到分析完gostartcallfn函数才知道
    newg.sched.pc = funcPC(goexit) + sys.PCQuantum // +PCQuantum so that previous instruction is in same function
    newg.sched.g = guintptr(unsafe.Pointer(newg))

    gostartcallfn(&newg.sched, fn) //调整sched成员和newg的栈
```

这段代码是将newg的sched初始化，(ps.注意区分sched 和 全局遍历schedt)。

1. 填充 sched.sp。 sp = stach.hi - totalszie。 hi是栈帧的位置，然后减去当前栈的大小就是栈顶的位置
2. 填充sched.pc。pc是当我们运行g的时候，加载的第一行命令，这里我们看到pc设置的不是fn的地址，而是goexit。是为什么呢？要回答这个问题，必须深入到gostartcallfn函数中做进一步分析。

```go
// adjust Gobuf as if it executed a call to fn
// and then did an immediate gosave.
func gostartcallfn(gobuf *gobuf, fv *funcval) {
    var fn unsafe.Pointer
    if fv != nil {
        fn = unsafe.Pointer(fv.fn) //fn: gorotine的入口地址，初始化时对应的是runtime.main
    } else {
        fn = unsafe.Pointer(funcPC(nilfunc))
    }
    gostartcall(gobuf, fn, unsafe.Pointer(fv))
}


// adjust Gobuf as if it executed a call to fn with context ctxt
// and then did an immediate gosave.
func gostartcall(buf *gobuf, fn, ctxt unsafe.Pointer) {
    sp := buf.sp //newg的栈顶，目前newg栈上只有fn函数的参数，sp指向的是fn的第一参数
    if sys.RegSize > sys.PtrSize {
        sp -= sys.PtrSize
        *(*uintptr)(unsafe.Pointer(sp)) = 0
    }
    sp -= sys.PtrSize //为返回地址预留空间，
    //这里在伪装fn是被goexit函数调用的，使得fn执行完后返回到goexit继续执行，从而完成清理工作
    *(*uintptr)(unsafe.Pointer(sp)) = buf.pc //在栈上放入goexit+1的地址
    buf.sp = sp //重新设置newg的栈顶寄存器
    //这里才真正让newg的ip寄存器指向fn函数，注意，这里只是在设置newg的一些信息，newg还未执行，
    //等到newg被调度起来运行时，调度器会把buf.pc放入cpu的IP寄存器，
    //从而使newg得以在cpu上真正的运行起来
    buf.pc = uintptr(fn)
    buf.ctxt = ctxt
}
```

gostartcallfn -> gostartcall。我们在gostartcall 中多了一些骚操作

1. 调整newg的栈空间，把goexit函数的第二条指令地址入栈，伪造成goexit函数调用了fn，从而当fn执行完之后，ret指令会执行goexit函数，从而可以完成清理工作。（这里比较骚，可以参考上文中[函数调用过程](#6. 函数调用过程)，里面有提到函数栈之前的关系）。有一点需要注意的，设置为goexit之后，相当于我们之前的变量都属于goexit栈帧了。
2. 重新设置newg.buf.pc为真正fn的地址。

#### putnewg 准备开始调度

继续往下看

```go
 		newg.gopc = callerpc  //主要用于traceback
    newg.ancestors = saveAncestors(callergp)
    //设置newg的startpc为fn.fn，该成员主要用于函数调用栈的traceback和栈收缩
    //newg真正从哪里开始执行并不依赖于这个成员，而是sched.pc
    newg.startpc = fn.fn  

    ......
   
    //设置g的状态为_Grunnable，表示这个g代表的goroutine可以运行了
    casgstatus(newg, _Gdead, _Grunnable)

    ......
   
    //把newg放入_p_的运行队列，初始化的时候一定是p的本地运行队列，其它时候可能因为本地队列满了而放入全局队列
    runqput(_p_, newg, true)

    ......
}
```

newproc1最后这些代码首先设置了gopc、startpc这些变量（与调度无关），然后修改newg状态为 `_Grunnable`，并将其放到可运行队列，至此第一个真正意义上的goroutine完成。

> Q:为什么说这个是第一个真正意思上的goroutine，而不是g0
>
> A:因为g0运行的条件和权限和是这个不同的，以后我们代码里创建的`g`,无论创建调度还有销毁和上面这个类似的，而不是和`g0`类似。（ps. 销毁不尽相同，后面会补充）

此时状态如图：

<img src="http://picgo.vipkk.work/20200517174402.png" alt="image-20200517174402452" style="zoom:50%;" />

这个图比较复杂，简单解释下

1. 首先，main g对应的newg结构体中的sched成员已经完成了初始化，图中展示了`pc`和`sp`2个成员。其中`pc`指向了`runtime.main`函数的第一条指令,sp指向了`newg`的栈顶内存单元，该内存单元保存了 goexit的第二条执行，也就是当main执行完成则会执行 `call runtime.goexit1(SB)`
2. newg已经放入与当前主线程绑定的p结构体对象的本地运行队列，因为它是第一个真正意义上的goroutine，还没有其它goroutine，所以它被放在了本地运行队列的头部；
3. newg的m成员为nil，因为它还没有被调度起来运行，也就没有跟任何m进行绑定。

## 4. 把newg放到CPU的汪洋大海中历练

> 关注以下三个问题
>
> 1. 如何保存g0的调度信息
> 2. schedule函数有什么重要作用
> 3. `gogo`函数如何从g0到main goroutine的切换

### 初始化

开始

```assembly
	MOVQ	$runtime·mainPC(SB), AX		// entry
	PUSHQ	AX  // newproc 的第二个参数入栈，也就算goroutine需要执行的函数。 AX = funcval{runtime.main}
	PUSHQ	$0			// arg size // newporc 的第一个参数入栈，该参数表示 runtime.main 函数需要的参数大小。 因为 runtime.main没有参数，所以这里是0
	CALL	runtime·newproc(SB) // 创建 main goroutine
	POPQ	AX
	POPQ	AX

	// start this M
	CALL	runtime·mstart(SB)

	CALL	runtime·abort(SB)	// mstart should never return
	RET
```

rto_go 在执行newproc后开始执行mstart，这里讲要开始我们的main之旅

> go的工程师就是喜欢这样，mstart没干什么，然后在`mstart1`中完成大部分工作

```go

// mstart is the entry-point for new Ms.
//
// This must not split the stack because we may not even have stack
// bounds set up yet.
//
// May run during STW (because it doesn't have a P yet), so write
// barriers are not allowed.
//
//go:nosplit
//go:nowritebarrierrec
func mstart() {
	_g_ := getg()

	osStack := _g_.stack.lo == 0
	if osStack {
		// Initialize stack bounds from system stack.
		// Cgo may have left stack size in stack.hi.
		// minit may update the stack bounds.
		size := _g_.stack.hi
		if size == 0 {
			size = 8192 * sys.StackGuardMultiplier
		}
		_g_.stack.hi = uintptr(noescape(unsafe.Pointer(&size)))
		_g_.stack.lo = _g_.stack.hi - size + 1024
	}
	// Initialize stack guard so that we can start calling regular
	// Go code.
	_g_.stackguard0 = _g_.stack.lo + _StackGuard
	// This is the g0, so we can also call go:systemstack
	// functions, which check stackguard1.
	_g_.stackguard1 = _g_.stackguard0
	mstart1()

	// Exit this thread.
	switch GOOS {
	case "windows", "solaris", "illumos", "plan9", "darwin", "aix":
		// Windows, Solaris, illumos, Darwin, AIX and Plan 9 always system-allocate
		// the stack, but put it in _g_.stack before mstart,
		// so the logic above hasn't set osStack yet.
		osStack = true
	}
	mexit(osStack)
}
```

mstart1

```go
func mstart1() {
    _g_ := getg()  //启动过程时 _g_ = m0的g0
    if _g_ != _g_.m.g0 {
        throw("bad runtime·mstart")
    }
    // Record the caller for use as the top of stack in mcall and
    // for terminating the thread.
    // We're never coming back to mstart1 after we call schedule,
    // so other calls can reuse the current frame.
    //getcallerpc()获取mstart1执行完的返回地址
    //getcallersp()获取调用mstart1时的栈顶地址
    save(getcallerpc(), getcallersp())
    asminit()  //在AMD64 Linux平台中，这个函数什么也没做，是个空函数
    minit()    //与信号相关的初始化，目前不需要关心
    // Install signal handlers; after minit so that minit can
    // prepare the thread to be able to handle the signals.
    if _g_.m == &m0 { //启动时_g_.m是m0，所以会执行下面的mstartm0函数
        mstartm0() //也是信号相关的初始化，现在我们不关注
    }
    if fn := _g_.m.mstartfn; fn != nil { //初始化过程中fn == nil
        fn()
    }
    if _g_.m != &m0 {// m0已经绑定了allp[0]，不是m0的话还没有p，所以需要获取一个p
        acquirep(_g_.m.nextp.ptr())
        _g_.m.nextp = 0
    }
    
    //schedule函数永远不会返回
    schedule()
}
```

### 莫名其妙的save

`mstart1`首先调用save函数来保存g0的调度信息，**save这一行代码非常非常重要，是我们理解调度循环的关键点之一**。首先mstart的参数分别是`mstart1执行完的返回地址`、`调用mstart1时的栈顶地址`.我们来看save的代码

```go
// save updates getg().sched to refer to pc and sp so that a following
// gogo will restore pc and sp.
//
// save must not have write barriers because invoking a write barrier
// can clobber getg().sched.
//
//go:nosplit
//go:nowritebarrierrec
func save(pc, sp uintptr) {
    _g_ := getg()
    _g_.sched.pc = pc //再次运行时的指令地址
    _g_.sched.sp = sp //再次运行时到栈顶
    _g_.sched.lr = 0
    _g_.sched.ret = 0
    _g_.sched.g = guintptr(unsafe.Pointer(_g_))
    // We need to ensure ctxt is zero, but can't have a write
    // barrier here. However, it should always already be zero.
    // Assert that.
    if _g_.sched.ctxt != nil {
        badctxt()
    }
}
```

`save`函数保存了调度相关的所有信息，包括sp、pc. 这个对于我们后面的协程调度是至关重要的。此时程序状态如图：

<img src="http://picgo.vipkk.work/20200521202355.png" alt="image-20200521202350334" style="zoom:50%;" />



> 这里其实是有疑问的，
>
> 1. 为什么g0已经执行到了mstart1这个函数而且还有后续调用其他函数，但是g0的调度信息中的pc和sp却要设置在mstart函数中。
> 2. mstart 后面的代码是if语句，且马上就要退出线程，为什么要这么做。
>
> 这些后续后解答，先继续看代码

### 调度开始

继续分析代码，save函数执行完成后，返回到mstart1继续 完成一些其他初始化，再然后就是 核心的 schedule函数了。

```go
// One round of scheduler: find a runnable goroutine and execute it.
// Never returns.
func schedule() {
    _g_ := getg()  //_g_ = 每个工作线程m对应的g0，初始化时是m0的g0
    //......
    var gp *g
  
    //......
    
    if gp == nil {
        // Check the global runnable queue once in a while to ensure fairness.
        // Otherwise two goroutines can completely occupy the local runqueue
        // by constantly respawning each other.
        //为了保证调度的公平性，每进行61次调度就需要优先从全局运行队列中获取goroutine，
        //因为如果只调度本地队列中的g，那么全局运行队列中的goroutine将得不到运行
        if _g_.m.p.ptr().schedtick%61 == 0 && sched.runqsize > 0 {
            lock(&sched.lock) //所有工作线程都能访问全局运行队列，所以需要加锁
            gp = globrunqget(_g_.m.p.ptr(), 1) //从全局运行队列中获取1个goroutine
            unlock(&sched.lock)
        }
    }
    if gp == nil {
        //从与m关联的p的本地运行队列中获取goroutine
        gp, inheritTime = runqget(_g_.m.p.ptr())
        if gp != nil && _g_.m.spinning {
            throw("schedule: spinning with local work")
        }
    }
    if gp == nil {
        //如果从本地运行队列和全局运行队列都没有找到需要运行的goroutine，
        //则调用findrunnable函数从其它工作线程的运行队列中偷取，如果偷取不到，则当前工作线程进入睡眠，
        //直到获取到需要运行的goroutine之后findrunnable函数才会返回。
      gp, inheritTime = findrunnable() // blocks until work is available
    }
    //跟启动无关的代码.....
    //当前运行的是runtime的代码，函数调用栈使用的是g0的栈空间
    //调用execte切换到gp的代码和栈空间去运行
    execute(gp, inheritTime)  
}
```

`schedule`的目的是竭尽可能的找到一个可以运行的`G`,然后调用`execute`去执行它，

而`execute`则是真正将newg放到cpu上执行的人。

```go
func execute(gp *g, inheritTime bool) {
    _g_ := getg() // 注意！！！！ 此时的_g_ 是g0
    //设置待运行g的状态为_Grunning
    casgstatus(gp, _Grunnable, _Grunning)
  
    //......
    
    //把g和m关联起来
    _g_.m.curg = gp 
    gp.m = _g_.m
    //......
    //gogo完成从g0到gp真正的切换
    gogo(&gp.sched)
}
```

`execute`函数的第一个参数是需要调度起来的goroutine，首先把他的状态切换为running，然后再把gp和m关联起来。

完成这些之后，execute调用gogo函数完成从g0到gp的栈切换：CPU执行权的转让以及栈的切换。

### 开始奔腾吧，切换到gp栈

> 对于栈的操作，还是汇编顺手

```assembly
// func gogo(buf *gobuf)
// restore state from Gobuf; longjmp
TEXT runtime·gogo(SB), NOSPLIT, $16-8
	MOVQ	buf+0(FP), BX		// gobuf bx=fp -> bx=gobuf
	MOVQ	gobuf_g(BX), DX // dx = gobuf.g
	MOVQ	0(DX), CX		// make sure g != nil
	get_tls(CX) // tls ... cx 的地址就是 tls[1]的地址，也是fs的地址 TODO 存疑
	MOVQ	DX, g(CX) // tls赋值，tls[0]设置为curg
	MOVQ	gobuf_sp(BX), SP	// restore SP
	MOVQ	gobuf_ret(BX), AX
	MOVQ	gobuf_ctxt(BX), DX
	MOVQ	gobuf_bp(BX), BP
	MOVQ	$0, gobuf_sp(BX)	// clear to help garbage collector
	MOVQ	$0, gobuf_ret(BX)
	MOVQ	$0, gobuf_ctxt(BX)
	MOVQ	$0, gobuf_bp(BX)
	MOVQ	gobuf_pc(BX), BX // bx = gobuf.pc
	JMP	BX // jmp bx
```

`gogo`这段代码短小精悍，

1. 把gp.sched的成员恢复到CPU寄存器中，并且通过sp完成栈的切换
2. 跳转到pc的指令地址

这里还有几点细节

1. 当把gp.sched恢复之后，gp 的sched被清空，减少gc。真是贴心。

此时的pc指向的是runtime.main。

> 还记得我们当初怎么创建人家的吗, 就是这样 pc指向runtime.main
>
> ```assembly
> 	// create a new goroutine to start program
> 	MOVQ	$runtime·mainPC(SB), AX		// entry
> 	PUSHQ	AX
> 	PUSHQ	$0			// arg size
> 	CALL	runtime·newproc(SB)
> 	POPQ	AX
> 	POPQ	AX
> ```

接着cpu开始执行runtime.main

```go
// The main goroutine.

func main() {
    g := getg()  // g = main goroutine，不再是g0了
    ......
    // Max stack size is 1 GB on 64-bit, 250 MB on 32-bit.
    // Using decimal instead of binary GB and MB because
    // they look nicer in the stack overflow failure message.
    if sys.PtrSize == 8 { //64位系统上每个goroutine的栈最大可达1G
        maxstacksize = 1000000000
    } else {
        maxstacksize = 250000000
    }
    // Allow newproc to start new Ms.
    mainStarted = true
    if GOARCH != "wasm" { // no threads on wasm yet, so no sysmon
        //现在执行的是main goroutine，所以使用的是main goroutine的栈，需要切换到g0栈去执行newm()
        systemstack(func() {
            //创建监控线程，该线程独立于调度器，不需要跟p关联即可运行
             newm(sysmon, nil)
        })
    }
    
    ......
    //调用runtime包的初始化函数，由编译器实现
    runtime_init() // must be before defer
    // Record when the world started.
    runtimeInitTime = nanotime()
    gcenable()  //开启垃圾回收器
    ......
    //main 包的初始化函数，也是由编译器实现，会递归的调用我们import进来的包的初始化函数
    fn := main_init // make an indirect call, as the linker doesn't know the address of the main package when laying down the runtime
    fn()
    ......
    
    //调用main.main函数
    fn = main_main // make an indirect call, as the linker doesn't know the address of the main package when laying down the runtime
    fn()
    
    ......
    //进入系统调用，退出进程，可以看出main goroutine并未返回，而是直接进入系统调用退出进程了
    exit(0)
    
    //保护性代码，如果exit意外返回，下面的代码也会让该进程crash死掉
    for {
        var x *int32
        *x = 0
    }
}
```

main函数主要工作如下：

1. 启动sysmon, 用来负责gc、抢占调度以及netpoll的监控等。
2. 执行runtime包的初始化 `runtime_init`
3. 执行main包初始化`main_init`
4. 开始执行
5. exit返回

至此流程结束，简单总结下g0是如何切换到maing上的

1. 首先save保存了当前的调度信息等到g0.sched
2. 通过调用schedule找到一个可以执行的goroutine
3. 调用execute完成maing和m的关联
4. gogo完成栈的切换
5. 开始执行知道exit(0)正常退出。



到这，大家是否还记得我们的save、我们的[exit的骚操作](#实例化sched--goexit的骚操作)，好像没什么用啊。直接exit了，一切回到解放前了，我们辛辛苦苦的save、exit貌似没什么用啊。

但是我们 代码里不仅有眼前的`maing`，还有其他`g`啊，他们的调度起始也是这样的呀，虽然mian g不再返还，但是其他g总会有结束的嘛，那时候就是我们save和exit的用武之地了。（ps. exit和save也是为他们设计的）



### 非main g的退出

假设现在有了第二个g称之为`g2`,我们看下它是怎么退出的

现在我们回到 goexit，来看看非 mian g的如何退出

```assembly
// The top-most function running on a goroutine
// returns to goexit+PCQuantum.
TEXT runtime·goexit(SB),NOSPLIT,$0-0
    BYTE  $0x90  // NOP
    CALL  runtime·goexit1(SB)  // does not return
    // traceback from goexit1 must hit code range of goexit
    BYTE  $0x90  // NOP
```

```go
// Finishes execution of the current goroutine.
func goexit1() {
	if raceenabled {
		racegoend()
	}
	if trace.enabled {
		traceGoEnd()
	}
	mcall(goexit0)
}
```

```assembly
// func mcall(fn func(*g))
// Switch to m->g0's stack, call fn(g).
// Fn must never return. It should gogo(&g->sched)
// to keep running g.
TEXT runtime·mcall(SB), NOSPLIT, $0-8
  // 取函数地址 fn -> DI
	MOVQ	fn+0(FP), DI // fn只是助记， DI = &fn

	get_tls(CX) // tls[1] -> CX
	MOVQ	g(CX), AX	// save state in g->sched tls[0]=g -> AX AX=tls[0],current g
	MOVQ	0(SP), BX	// caller's PC sp-> BX BX=SP
	MOVQ	BX, (g_sched+gobuf_pc)(AX)  // curg.pc = BX = SP
	LEAQ	fn+0(FP), BX	// caller's SP
	MOVQ	BX, (g_sched+gobuf_sp)(AX) // sp
	MOVQ	AX, (g_sched+gobuf_g)(AX) // g
	MOVQ	BP, (g_sched+gobuf_bp)(AX) //bp

	// switch to m->g0 & its stack, call fn
	MOVQ	g(CX), BX // BX = g
	MOVQ	g_m(BX), BX // bx=g.m
	MOVQ	m_g0(BX), SI // si = g.m.g0 好吧终于到g0了，此时si 是g0,bx是curg
	CMPQ	SI, AX	// if g == m->g0 call badmcall
	JNE	3(PC)
	MOVQ	$runtime·badmcall(SB), AX
	JMP	AX
	MOVQ	SI, g(CX)	// g = m->g0，终于到了这一刻，把g0放到tls，从此切换了到了g0,curg自此无缘。
	MOVQ	(g_sched+gobuf_sp)(SI), SP	// 开始恢复寄存器， sp = g.sp
	PUSHQ	AX // 此时AX还是curg, pushq 入栈操作，多么熟悉要进行函数调用了
	MOVQ	DI, DX // 默默无闻的DI出现了，他是我们mcall的参数啊，他是fn，他又不是fn,他是 一个结构体里面是fn
	MOVQ	0(DI), DI // 读取 DI第一个成员，这才是fn
	CALL	DI // 调用fn，也就是goexit
	POPQ	AX
	MOVQ	$runtime·badmcall2(SB), AX
	JMP	AX
	RET
```

由上，调用链条是`ret -> goexit -> goexit1 -> mcall(goexit0)-> goexit0`

`g2`返回之后因为之前的骚操作，执行goexit代码的第二行` CALL  runtime·goexit1(SB) `,从而执行到`goexit1`,`goexit1`呢也只是简单做下检查，就调用了mcall，将栈切换到了g0，然后再g0栈上执行goexit0(gp)这个函数。

mcall这个函数比较有意思，它基本上就是我们之前`gogo`的逆操作。具体的解释已经在上面了。

> 有一个点需要注意下，在Go语言实现中，函数变量并不是一个直接指向函数代码的指针，而是一个直系那个funcval结构体对象的指针，而funcval对象的第一个成员fn才是真正指向函数代码的指针。
>
> ```go
> type funcval struct {
> 	fn uintptr
> 	// variable-size, fn-specific data here
> }
> ```
>
> 这也就是为什么mcall中 `movq 0(DI), DI`了。

mcall函数主要有2个功能

1. 保存curg信息，切换到g0栈
2. 在g0栈上 调用 fn(curg)。

> 一个比喻，
>
> 在curg上执行 mcall(fn)，就像是脖子一伸，然后告诉g0,你用这个fn来处理我把。



上面提到`mcall`做的事情就像是`gogo`的逆函数。但是顺序还是有不一样的

- `gogo`函数从g0切换到其他goroutine的时候首先切换了栈，然后通过`jmp`指令从runtime代码切换到了用户`goroutine`的代码去执行
- `mcall`呢只是切换了栈，然后就直接`call`到了ax

原因：从g0切换到其它goroutine之前执行的是runtime的代码而且使用的是g0栈，所以切换时需要首先切换栈然后再从runtime代码跳转某个goroutine的代码去执行（切换栈和跳转指令不能颠倒，因为跳转之后执行的就是用户的goroutine代码了，没有机会切换栈了），然而从某个goroutine切换回g0时，goroutine使用的是call指令来调用mcall函数，mcall函数本身就是runtime的代码，所以call指令其实已经完成了从goroutine代码到runtime代码的跳转，因此mcall函数自身的代码就不需要再跳转了，只需要把栈切换到g0栈即可。



使用mcall切换到g0栈之后，开始执行goexit0函数

```go
// goexit continuation on g0.
func goexit0(gp *g) {
	_g_ := getg()

	casgstatus(gp, _Grunning, _Gdead)
	if isSystemGoroutine(gp, false) {
		atomic.Xadd(&sched.ngsys, -1)
	}
	gp.m = nil
	locked := gp.lockedm != 0
	gp.lockedm = 0
	_g_.m.lockedg = 0
	gp.preemptStop = false
	gp.paniconfault = false
	gp._defer = nil // should be true already but just in case.
	gp._panic = nil // non-nil for Goexit during panic. points at stack-allocated data.
	gp.writebuf = nil
	gp.waitreason = 0
	gp.param = nil
	gp.labels = nil
	gp.timer = nil

	...

	dropg()

	...
  
	gfput(_g_.m.p.ptr(), gp)
	
  ...
	
  schedule()
}
```

goexit函数主要做了下面几个事情

1. 将g(这个g是之前退出的g)状态切换到_Gdead
2. 把g的字段清空
3. 调用dropg把g和m之间的关系解除
4. 把g放到freeg队列缓存起来，（对象池）
5. 调用schedule再次调度。



### 神奇的调度循环

此时我们稍微整理下，可以发现大概的调度是这样的

```shell
mstart->mstart1->schedule->execute->gogo->otherg->goexit1->mcall->goexit0->shecdule
```

可以看出，一轮调度是从调度schedule函数开始，然后经过一系列代码的执行到最后又再次调用了schedule函数来进行一轮新的调度，这个一轮我们称为一次调度循环。并且这个循环是针对某一个工作线程的。而一个go程序可能存在多个工作线程，每个工作线程都有自己的调度循环。

> 这是调度没错，但是没有循环啊，一直没有返回，奔腾不复回。

我们上面虽然调度是循环的，但是一直没有return，这样的函数栈不会耗尽吗？ 这就又回到了我们最开始的[save](###莫名其妙的save)操作。因为每次mcall的时候，g0栈都是指向固定的g0.sp位置，而g0.sp早就设置好了。如下图：



<img src="http://picgo.vipkk.work/20200522221059.png" alt="image-20200522221059610" style="zoom:50%;" />

工作线程执行流程总结：

1. 初始化，调用mstart
2. 调用mstart1，在该函数中调用save函数保存g0等调度信息，其中g0.sched.sp指向mstart函数栈帧的栈顶；
3. 次调用schedule->execute->gogo函数执行调度；
4. 运行用户的goroutine代码；
5. 用户goroutine代码执行过程中调用runtime中的某些函数，然后这些函数调用mcall切换到g0.sched.sp所指的栈并最终再次调用schedule函数进入新一轮调度，之后工作线程一直循环执行着3～5这一调度循环直到进程退出为止。

